{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd: automatic differentiation\n",
    "\n",
    "The ``autograd`` package provides automatic differentiation for all operations\n",
    "on Tensors. It is a define-by-run framework, which means that your backprop is\n",
    "defined by how your code is run, and that every single iteration can be\n",
    "different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Create a 2x2 tensor with gradient-accumulation capabilities\n",
    "x = torch.tensor([[1, 2], [3, 4]], requires_grad=True, dtype=torch.float32)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do an operation on the tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.,  0.],\n",
      "        [ 1.,  2.]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Deduct 2 from all elements\n",
    "y = x - 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``y`` was created as a result of an operation, so it has a ``grad_fn``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SubBackward0 object at 0x130c38bb0>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# What's happening here?\n",
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SubBackward0 at 0x130c397e0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's dig further...\n",
    "y.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AccumulateGrad at 0x130c38cd0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad_fn.next_functions[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad_fn.next_functions[0][0].variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.,  0.],\n",
      "        [ 3., 12.]], grad_fn=<MulBackward0>)\n",
      "tensor(4.5000, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Do more operations on y\n",
    "z = y * y * 3\n",
    "a = z.mean()  # average\n",
    "\n",
    "print(z)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualise the computational graph! (thks @szagoruyko)\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !brew install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !echo 'export PATH=\"/opt/homebrew/opt/openjdk/bin:$PATH\"' >> ~/.zshrc\n",
    "# !echo 'export CPPFLAGS=\"-I/opt/homebrew/opt/openjdk/include\"' >> ~/.zshrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 8.0.1 (20230327.1645)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"109pt\" height=\"381pt\"\n",
       " viewBox=\"0.00 0.00 109.00 381.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 377)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-377 105,-377 105,4 -4,4\"/>\n",
       "<!-- 4434237440 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>4434237440</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"77.5,-31 23.5,-31 23.5,0 77.5,0 77.5,-31\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n",
       "</g>\n",
       "<!-- 5127969792 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>5127969792</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"98,-86 3,-86 3,-67 98,-67 98,-86\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">MeanBackward0</text>\n",
       "</g>\n",
       "<!-- 5127969792&#45;&gt;4434237440 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>5127969792&#45;&gt;4434237440</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.5,-66.54C50.5,-59.99 50.5,-50.77 50.5,-42.02\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54,-42.06 50.5,-32.06 47,-42.06 54,-42.06\"/>\n",
       "</g>\n",
       "<!-- 5127969360 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>5127969360</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"95,-141 6,-141 6,-122 95,-122 95,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 5127969360&#45;&gt;5127969792 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>5127969360&#45;&gt;5127969792</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.5,-121.75C50.5,-115.11 50.5,-105.73 50.5,-97.31\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54,-97.44 50.5,-87.44 47,-97.44 54,-97.44\"/>\n",
       "</g>\n",
       "<!-- 5127969552 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>5127969552</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"95,-196 6,-196 6,-177 95,-177 95,-196\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 5127969552&#45;&gt;5127969360 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>5127969552&#45;&gt;5127969360</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.5,-176.75C50.5,-170.11 50.5,-160.73 50.5,-152.31\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54,-152.44 50.5,-142.44 47,-152.44 54,-152.44\"/>\n",
       "</g>\n",
       "<!-- 5113092064 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>5113092064</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"95,-251 6,-251 6,-232 95,-232 95,-251\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\">SubBackward0</text>\n",
       "</g>\n",
       "<!-- 5113092064&#45;&gt;5127969552 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>5113092064&#45;&gt;5127969552</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M45.33,-231.75C43.91,-225.11 43.44,-215.73 43.94,-207.31\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"47.54,-207.8 45.29,-197.44 40.59,-206.94 47.54,-207.8\"/>\n",
       "</g>\n",
       "<!-- 5113092064&#45;&gt;5127969552 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>5113092064&#45;&gt;5127969552</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M55.67,-231.75C57.09,-225.11 57.56,-215.73 57.06,-207.31\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"60.41,-206.94 55.71,-197.44 53.46,-207.8 60.41,-206.94\"/>\n",
       "</g>\n",
       "<!-- 5113089232 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5113089232</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"101,-306 0,-306 0,-287 101,-287 101,-306\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-294\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 5113089232&#45;&gt;5113092064 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>5113089232&#45;&gt;5113092064</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.5,-286.75C50.5,-280.11 50.5,-270.73 50.5,-262.31\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54,-262.44 50.5,-252.44 47,-262.44 54,-262.44\"/>\n",
       "</g>\n",
       "<!-- 4433525024 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>4433525024</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"80,-373 21,-373 21,-342 80,-342 80,-373\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-349\" font-family=\"monospace\" font-size=\"10.00\"> (2, 2)</text>\n",
       "</g>\n",
       "<!-- 4433525024&#45;&gt;5113089232 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>4433525024&#45;&gt;5113089232</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.5,-341.61C50.5,-334.18 50.5,-325.08 50.5,-317.09\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54,-317.16 50.5,-307.16 47,-317.16 54,-317.16\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x131a69bd0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dot(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients\n",
    "\n",
    "Let's backprop now `out.backward()` is equivalent to doing `out.backward(torch.tensor([1.0]))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = y.retain_grad(), z.retain_grad(), a.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backprop\n",
    "a.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print gradients $\\frac{\\text{d}a}{\\text{d}x}$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad: tensor([[-1.5000,  0.0000],\n",
      "        [ 1.5000,  3.0000]]),\n",
      "y.grad: tensor([[-1.5000,  0.0000],\n",
      "        [ 1.5000,  3.0000]]),\n",
      "z.grad: tensor([[0.2500, 0.2500],\n",
      "        [0.2500, 0.2500]]),\n",
      "a.grad: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Compute it by hand BEFORE executing this\n",
    "print(f'x.grad: {x.grad},\\ny.grad: {y.grad},\\nz.grad: {z.grad},\\na.grad: {a.grad}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:\n",
    "\n",
    "z.grad is [[0.25]*2]*2 because mean is computed as sum(elements)/count(elements) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do many crazy things with autograd!\n",
    "> With Great *Flexibility* Comes Great Responsibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 684.7039, 1176.0471, 1418.2847], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Dynamic graphs!\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "i = 0\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "    i += 1\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"
     ]
    }
   ],
   "source": [
    "# If we don't run backward on a scalar we need to specify the grad_output\n",
    "gradients = torch.FloatTensor([0.1, 1.0, 0.0001])\n",
    "y.backward(gradients)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "# BEFORE executing this, can you tell what would you expect it to print?\n",
    "print(i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation\n",
    "\n",
    "Grad output means The input dimensions are multiplied by the grad_output and summed up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This variable decides the tensor's range below\n",
    "n = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.])\n",
      "tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# Both x and w that allows gradient accumulation\n",
    "x = torch.arange(1., n + 1, requires_grad=True)\n",
    "w = torch.ones(n, requires_grad=True)\n",
    "z = w @ x\n",
    "z.backward()\n",
    "print(x.grad, w.grad, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# Only w that allows gradient accumulation\n",
    "x = torch.arange(1., n + 1)\n",
    "w = torch.ones(n, requires_grad=True)\n",
    "z = w @ x\n",
    "z.backward()\n",
    "print(x.grad, w.grad, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuntimeError!!! >:[\n",
      "element 0 of tensors does not require grad and does not have a grad_fn\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1., n + 1)\n",
    "w = torch.ones(n, requires_grad=True)\n",
    "\n",
    "# Regardless of what you do in this context, all torch tensors will not have gradient accumulation\n",
    "with torch.no_grad():\n",
    "    z = w @ x\n",
    "\n",
    "try:\n",
    "    z.backward()  # PyTorch will throw an error here, since z has no grad accum.\n",
    "except RuntimeError as e:\n",
    "    print('RuntimeError!!! >:[')\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More stuff\n",
    "\n",
    "Documentation of the automatic differentiation package is at\n",
    "http://pytorch.org/docs/autograd."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
